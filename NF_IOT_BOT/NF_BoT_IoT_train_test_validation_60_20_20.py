# -*- coding: utf-8 -*-
"""task.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EBat3y_lQtVzZCd8sxd0aWm1ZJV5V8Hy
"""

## **Importing Modules and Libraries**
import os
# Suppress TensorFlow GPU-related warnings
# importing required libraries
import numpy as np
import pandas as pd
import time

import seaborn as sns
import matplotlib.pyplot as plt

import pickle
from os import path


from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import Pipeline


from sklearn import metrics
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import multilabel_confusion_matrix
from sklearn.metrics import confusion_matrix

from sklearn.svm import SVC
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier, export_text
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import AdaBoostClassifier

from keras.models import Sequential
from keras.layers import Dense
import tensorflow as tf
import tensorflow

from tensorflow import keras
from keras.layers import Conv1D, MaxPooling1D, Dense, Flatten , Activation, SimpleRNN, LSTM, GRU, Dropout, TimeDistributed, Reshape, Input, Lambda, Add
from keras import Sequential

from tensorflow.keras import backend as K
from keras.models import Model
from keras.optimizers import Adam



import sklearn.discriminant_analysis
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.neural_network import BernoulliRBM
from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.linear_model import RidgeClassifier
from sklearn.datasets import make_classification
from sklearn.ensemble import BaggingClassifier
from sklearn.utils import class_weight
from sklearn.neighbors import NearestCentroid
from sklearn.mixture import GaussianMixture
from sklearn.metrics import accuracy_score
import skfuzzy as fuzz
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import SGDClassifier


from sklearn.preprocessing import label_binarize
from keras.utils import to_categorical
import warnings
warnings.simplefilter("ignore")


from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.decomposition import PCA
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, classification_report, multilabel_confusion_matrix
from sklearn.tree import DecisionTreeClassifier, export_text
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier, StackingClassifier, AdaBoostClassifier, HistGradientBoostingClassifier, IsolationForest, AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LinearRegression, LogisticRegression, PassiveAggressiveClassifier, RidgeClassifier, SGDClassifier
from sklearn.neighbors import KNeighborsClassifier, NearestCentroid
from sklearn.neural_network import MLPClassifier, BernoulliRBM
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis
from sklearn.datasets import make_classification
from sklearn.utils import class_weight
from sklearn.mixture import GaussianMixture
from sklearn.feature_selection import RFE
from sklearn.multioutput import MultiOutputClassifier
from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB
import sklearn.metrics as metrics

import skfuzzy as fuzz
from pgmpy.estimators import TreeSearch

from hmmlearn import hmm
from keras.models import Sequential, Model
from keras.layers import Conv1D, MaxPooling1D, Dense, Flatten, Activation, SimpleRNN, LSTM, GRU, Dropout, TimeDistributed, Reshape, Input, Lambda, Add
from keras.optimizers import Adam
from keras.utils import to_categorical
from mlxtend.frequent_patterns import apriori, association_rules
import warnings
warnings.filterwarnings("ignore")
from pgmpy.models import BayesianModel
from pomegranate import *
from pgmpy.models import BayesianModel
from pgmpy.models import JunctionTree
from skopt import BayesSearchCV
from skopt.space import Real, Categorical, Integer
from catboost import CatBoostClassifier
import tensorflow as tf
from pgmpy.models import BayesianNetwork
from pgmpy.estimators import ExpectationMaximization
from sklearn.naive_bayes import GaussianNB
from sklearn.decomposition import FastICA
from sklearn.preprocessing import StandardScaler
from sklearn.gaussian_process.kernels import RBF
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.manifold import TSNE
from sklearn.ensemble import RandomForestClassifier
from tensorflow.keras.utils import to_categorical
from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.datasets import make_classification
from pgmpy.models import MarkovModel
from pgmpy.estimators import MaximumLikelihoodEstimator
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
import torch
from torch.utils.data import TensorDataset,DataLoader
import torch.nn as nn
import torchvision
from torch import optim
from skopt import BayesSearchCV
from aco import AntColony
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import multilabel_confusion_matrix, confusion_matrix
from sklearn.ensemble import VotingClassifier


# requried to change for each dataset
method = "NF-BOT-IOT_train_No_Opt_No_Smote"

# dont change this
method = method + "_train_test_validation_60_20_20_Metrics"

## **Importing Datasets**
# # 1.NO_Opt
train_data = pd.read_csv('NF-BOT-IOT_train_No_Opt_No_Smote.csv')
# train_data = pd.read_csv('NF-BOT-IOT_train_No_Opt_Smote_ENN.csv')
# train_data = pd.read_csv('NF-BOT-IOT_train_No_Opt_Smote_IPF.csv')

test_data = pd.read_csv('NF-BOT-IOT_test_No_Opt.csv')

# # 2.BBA
# train_data = pd.read_csv('NF-BOT-IOT_train_BBA_No_Smote.csv')
# train_data = pd.read_csv('NF-BOT-IOT_train_BBA_Smote_ENN.csv')
# train_data = pd.read_csv('NF-BOT-IOT_train_BBA_Smote_IPF.csv')

# test_data = pd.read_csv('NF-BOT-IOT_test_BBA.csv')

# # 3.CS
# train_data = pd.read_csv('NF-BOT-IOT_train_CS_No_Smote.csv')
# train_data = pd.read_csv('NF-BOT-IOT_train_CS_Smote_ENN.csv')
# train_data = pd.read_csv('NF-BOT-IOT_train_CS_Smote_IPF.csv')

# test_data = pd.read_csv('NF-BOT-IOT_test_CS.csv')

# # 4.NO_Opt
# train_data = pd.read_csv('NF-BOT-IOT_train_DE_No_Smote.csv')
# train_data = pd.read_csv('NF-BOT-IOT_train_DE_Smote_ENN.csv')
# train_data = pd.read_csv('NF-BOT-IOT_train_DE_Smote_IPF.csv')

# test_data = pd.read_csv('NF-BOT-IOT_test_DE.csv')

# # 5.EO
# train_data = pd.read_csv('NF-BOT-IOT_train_EO_No_Smote.csv')
# train_data = pd.read_csv('NF-BOT-IOT_train_EO_Smote_ENN.csv')
# train_data = pd.read_csv('NF-BOT-IOT_train_EO_Smote_IPF.csv')

# test_data = pd.read_csv('NF-BOT-IOT_test_EO.csv')

# # 6.FA
# train_data = pd.read_csv('NF-BOT-IOT_train_FA_No_Smote.csv')
# train_data = pd.read_csv('NF-BOT-IOT_train_FA_Smote_ENN.csv')
# train_data = pd.read_csv('NF-BOT-IOT_train_FA_Smote_IPF.csv')

# test_data = pd.read_csv('NF-BOT-IOT_test_FA.csv')

# # 7.FPA
# train_data = pd.read_csv('NF-BOT-IOT_train_FPA_No_Smote.csv')
# train_data = pd.read_csv('NF-BOT-IOT_train_FPA_Smote_ENN.csv')
# train_data = pd.read_csv('NF-BOT-IOT_train_FPA_Smote_IPF.csv')

# test_data = pd.read_csv('NF-BOT-IOT_test_FPA.csv')

# # 8.GA
# train_data = pd.read_csv('NF-BOT-IOT_train_GA_No_Smote.csv')
# train_data = pd.read_csv('NF-BOT-IOT_train_GA_Smote_ENN.csv')
# train_data = pd.read_csv('NF-BOT-IOT_train_GA_Smote_IPF.csv')

# test_data = pd.read_csv('NF-BOT-IOT_test_GA.csv')

# # 9.GSA
# train_data = pd.read_csv('NF-BOT-IOT_train_GSA_No_Smote.csv')
# train_data = pd.read_csv('NF-BOT-IOT_train_GSA_Smote_ENN.csv')
# train_data = pd.read_csv('NF-BOT-IOT_train_GSA_Smote_IPF.csv')

# test_data = pd.read_csv('NF-BOT-IOT_test_GSA.csv')

# # 10.GWO
# train_data = pd.read_csv('NF-BOT-IOT_train_GWO_No_Smote.csv')
# train_data = pd.read_csv('NF-BOT-IOT_train_GWO_Smote_ENN.csv')
# train_data = pd.read_csv('NF-BOT-IOT_train_GWO_Smote_IPF.csv')

# test_data = pd.read_csv('NF-BOT-IOT_test_GWO.csv')

# # 11.HHO
# train_data = pd.read_csv('NF-BOT-IOT_train_HHO_No_Smote.csv')
# train_data = pd.read_csv('NF-BOT-IOT_train_HHO_SMOTE_ENN.csv')
# train_data = pd.read_csv('NF-BOT-IOT_train_HHO_Smote_IPF.csv')

# test_data = pd.read_csv('NF-BOT-IOT_test_HHO.csv')


# # 12.HS
# train_data = pd.read_csv('NF-BOT-IOT_train_HS_No_Smote.csv')
# train_data = pd.read_csv('NF-BOT-IOT_train_HS_Smote_ENN.csv')
# train_data = pd.read_csv('NF-BOT-IOT_train_HS_Smote_IPF.csv')

# test_data = pd.read_csv('NF-BOT-IOT_test_HS.csv')

# # 13.JA
# train_data = pd.read_csv('NF-BOT-IOT_train_JA_No_Smote.csv')
# train_data = pd.read_csv('NF-BOT-IOT_train_JA_Smote_ENN.csv')
# train_data = pd.read_csv('NF-BOT-IOT_train_JA_Smote_IPF.csv')

# test_data = pd.read_csv('NF-BOT-IOT_test_JA.csv')

# # 14.MA
# train_data = pd.read_csv('NF-BOT-IOT_train_MA_No_Smote.csv')
# train_data = pd.read_csv('NF-BOT-IOT_train_MA_Smote_ENN.csv')
# train_data = pd.read_csv('NF-BOT-IOT_train_MA_Smote_IPF.csv')

# test_data = pd.read_csv('NF-BOT-IOT_test_MA.csv')

# # 15.PSO
# train_data = pd.read_csv('NF-BOT-IOT_train_PSO_No_Smote.csv')
# train_data = pd.read_csv('NF-BOT-IOT_train_PSO_Smote_ENN.csv')
# train_data = pd.read_csv('NF-BOT-IOT_train_PSO_Smote_IPF.csv')

# test_data = pd.read_csv('NF-BOT-IOT_test_PSO.csv')

# # 16.RDA
# train_data = pd.read_csv('NF-BOT-IOT_train_RDA_No_Smote.csv')
# train_data = pd.read_csv('NF-BOT-IOT_train_RDA_Smote_ENN.csv')
# train_data = pd.read_csv('NF-BOT-IOT_train_RDA_Smote_IPF.csv')

# test_data = pd.read_csv('NF-BOT-IOT_test_RDA.csv')

# # 17.SCA
# train_data = pd.read_csv('NF-BOT-IOT_train_SCA_No_Smote.csv')
# train_data = pd.read_csv('NF-BOT-IOT_train_SCA_Smote_ENN.csv')
# train_data = pd.read_csv('NF-BOT-IOT_train_SCA_Smote_IPF.csv')

# test_data = pd.read_csv('NF-BOT-IOT_test_SCA.csv')

# # 18.SSA
# train_data = pd.read_csv('NF-BOT-IOT_train_SSA_No_Smote.csv')
# train_data = pd.read_csv('NF-BOT-IOT_train_SSA_Smote_ENN.csv')
# train_data = pd.read_csv('NF-BOT-IOT_train_SSA_Smote_IPF.csv')

# test_data = pd.read_csv('NF-BOT-IOT_test_SSA.csv')

# # 19.WOA
# train_data = pd.read_csv('NF-BOT-IOT_train_WOA_No_Smote.csv')
# train_data = pd.read_csv('NF-BOT-IOT_train_WOA_Smote_ENN.csv')
# train_data = pd.read_csv('NF-BOT-IOT_train_WOA_Smote_IPF.csv')


# test_data = pd.read_csv('UNSW_NB_15_test_WOA.csv')
# print(train_data.shape)
# print(test_data.shape)


# **MULTI-CLASS CLASSIFICATION**
## **Data Splitting**
X_train = train_data.drop(columns=['label'],axis=1)
X_test = test_data.drop(columns=['label'],axis=1)
y_train = train_data['label']
y_test = test_data['label']


print(X_train.shape)
X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=1/7, random_state=7)
print(X_test.shape)
print(X_valid.shape)
print(X_train.shape)

#feature_list = ['service','dpkts','dbytes','sttl','sload','dloss','sjit','smean','dmean','trans_depth','ct_state_ttl','ct_dst_ltm','ct_src_dport_ltm','ct_dst_sport_ltm','ct_dst_src_ltm','ct_ftp_cmd','ct_flw_http_mthd','ct_srv_dst','is_sm_ips_ports']
#X = X[feature_list]
fname = method + "_output.csv"
file = open(fname, 'w')
file.write("algo vs matrices,time to train(sec),time to predict(sec),accuracy_score,precision_score,recall_score,f1_score,fbeta_score,matthews_corrcoef,jaccard_score,cohen_kappa_score,hamming_loss,zero_one_loss,mean_absolute_error,mean_squared_error,mean_squared_error,balanced_accuracy_score,explained_variance_score,val_accuracy\n")
def format_decimal(number):
    return f"{number:.{3}f}"
def result(y_pred,y_test,algo,start,end_train,end_predict,val_accuracy):
    file.write(algo+",")
    file.write(str(format_decimal(end_train-start))+",")
    file.write(str(format_decimal(end_predict-end_train))+",")
    file.write(str(format_decimal(metrics.accuracy_score(y_test,y_pred)))+",")
    file.write(str(format_decimal(metrics.precision_score(y_test, y_pred, average='weighted')))+",")
    file.write(str(format_decimal(metrics.recall_score(y_test, y_pred, average='weighted')))+",")
    file.write(str(format_decimal(metrics.f1_score(y_test, y_pred, average='weighted')))+",")
    file.write(str(format_decimal(metrics.fbeta_score(y_test, y_pred,average='weighted', beta=0.5)))+",")
    file.write(str(format_decimal(metrics.matthews_corrcoef(y_test, y_pred)))+",")
    file.write(str(format_decimal(metrics.jaccard_score(y_test, y_pred, average='weighted')))+",")
    file.write(str(format_decimal(metrics.cohen_kappa_score(y_test, y_pred)))+",")
    file.write(str(format_decimal(metrics.hamming_loss(y_test, y_pred)))+",")
    file.write(str(format_decimal(metrics.zero_one_loss(y_test, y_pred)))+",")
    file.write(str(format_decimal(metrics.mean_absolute_error(y_test, y_pred)))+",")
    file.write(str(format_decimal(metrics.mean_squared_error(y_test, y_pred)))+",")
    file.write(str(format_decimal(np.sqrt(metrics.mean_squared_error(y_test, y_pred))))+",")
    file.write(str(format_decimal(metrics.balanced_accuracy_score(y_test, y_pred)))+",")
    file.write(str(format_decimal(metrics.explained_variance_score(y_test, y_pred)*100))+",")
    file.write(str(format_decimal(val_accuracy))+"\n")

## **1.Decision Tree**
dt_multi = DecisionTreeClassifier(random_state=24)
start = time.time()
dt_multi.fit(X_train,y_train)
end_train = time.time()
y_pred = dt_multi.predict(X_test)
end_predict = time.time()
y_val_pred = dt_multi.predict(X_valid)
val_accuracy = accuracy_score(y_valid, y_val_pred)
result(y_pred,y_test,"DT",start,end_train,end_predict,val_accuracy)
pname = method +"_Decision_Tree_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
#plt.show()
print("Decision Tree completed")

## **2.Linear Regression**
lr_multi = LinearRegression()
start = time.time()
lr_multi.fit(X_train, y_train)
end_train = time.time()
y_pred = lr_multi.predict(X_test)
end_predict = time.time()

for i in range(len(y_pred)):
  y_pred[i] = int(np.round_(y_pred[i]))

# Make predictions on validation set
y_val_pred = lr_multi.predict(X_valid)
for i in range(len(y_val_pred)):
  y_val_pred[i] = int(np.round_(y_val_pred[i]))
val_accuracy = accuracy_score(y_valid, y_val_pred)
result(y_pred,y_test,"Linear Regression",start,end_train,end_predict,val_accuracy)
pname = method +"_"+"Linear_Regression"+"_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
#plt.show()
print("Linear Regression completed")

## **3.Logistic Regression**
logr_multi = LogisticRegression(random_state=123, max_iter=5000,solver='newton-cg',multi_class='multinomial')
start = time.time()
logr_multi.fit(X_train,y_train)
end_train = time.time()
y_pred = logr_multi.predict(X_test)
end_predict = time.time()
y_val_pred = logr_multi.predict(X_valid)
val_accuracy = accuracy_score(y_valid, y_val_pred)
result(y_pred,y_test,"Logistic Regression",start,end_train,end_predict,val_accuracy)
pname = method +"_"+"Logistic_Regression"+"_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
#plt.show()
print("Logistic Regression completed")

## **4.K Nearest Neighbor Classifier**
knn_multi = KNeighborsClassifier(n_neighbors=5)
start = time.time()
knn_multi.fit(X_train,y_train)
end_train = time.time()
y_pred = knn_multi.predict(X_test)
end_predict = time.time()
y_val_pred = knn_multi.predict(X_valid)
val_accuracy = accuracy_score(y_valid, y_val_pred)
result(y_pred,y_test,"KNN",start,end_train,end_predict,val_accuracy)
pname = method +"_"+"KNeighborsClassifier"+"_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
#plt.show()
print("KNN completed")

## **5.Random Forest Classifier**
rf_multi = RandomForestClassifier(random_state=50)
start = time.time()
rf_multi.fit(X_train,y_train)
end_train = time.time()
y_pred = rf_multi.predict(X_test)
end_predict = time.time()
y_val_pred = rf_multi.predict(X_valid)
val_accuracy = accuracy_score(y_valid, y_val_pred)
result(y_pred,y_test,"Random Forest",start,end_train,end_predict,val_accuracy)
pname = method +"_"+"RandomForestClassifier"+"_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
#plt.show()
print("Random Forest Classifier completed")

## **6.Multi Layer Perceptron**

mlp_multi = MLPClassifier(random_state=123, solver='adam', max_iter=8000)
start = time.time()
mlp_multi.fit(X_train,y_train)
end_train = time.time()
y_pred = mlp_multi.predict(X_test)
end_predict = time.time()
y_val_pred = mlp_multi.predict(X_valid)
val_accuracy = accuracy_score(y_valid, y_val_pred)
result(y_pred,y_test,"MLP",start,end_train,end_predict,val_accuracy)
pname = method +"_"+"MLPClassifier"+"_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
#plt.show()
print("MLP completed")

## **7.Bagging**
# Create a base classifier (Decision Tree)
base_classifier = DecisionTreeClassifier(random_state=42)

# Create a bagging classifier
bagging_classifier = BaggingClassifier(base_classifier, n_estimators=10, random_state=42)
start = time.time()
# Train the bagging classifier
bagging_classifier.fit(X_train, y_train)
end_train = time.time()
# Predict on the test set
y_pred = bagging_classifier.predict(X_test)
end_predict = time.time()
y_val_pred = bagging_classifier.predict(X_valid)
val_accuracy = accuracy_score(y_valid, y_val_pred)
result(y_pred,y_test,"Bagging",start,end_train,end_predict,val_accuracy)
pname = method +"_"+"Bagging"+"_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
#plt.show()
print("Bagging completed")

## **8.J48**

# Initialize and train the J48 (Decision Tree) classifier
classifier = DecisionTreeClassifier(criterion='entropy', random_state=42)
start = time.time()
classifier.fit(X_train, y_train)
end_train = time.time()
# Make predictions
y_pred = classifier.predict(X_test)
end_predict = time.time()
y_val_pred = classifier.predict(X_valid)
val_accuracy = accuracy_score(y_valid, y_val_pred)
result(y_pred,y_test,"J48",start,end_train,end_predict,val_accuracy)
pname = method +"_"+"J48"+"_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
#plt.show()
print("J48 completed")

## **9.ANN**
multi_ann = Sequential()
# Adding the input layer and the first hidden layer
multi_ann.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = X_train.shape[1]))

# Adding the second hidden layer
multi_ann.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))

# Adding the output layer
multi_ann.add(Dense(units = len(np.unique(y_train)), kernel_initializer = 'uniform', activation = 'softmax'))

# Compiling the ANN | means applying SGD on the whole ANN
multi_ann.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])

# Fitting the ANN to the Training set
start = time.time()
history = multi_ann.fit(X_train, y_train,epochs=10, batch_size=50,verbose=2,validation_data=(X_valid,y_valid))
end_train = time.time()
# Predicting the Test set results
y_pred = np.argmax(multi_ann.predict(X_test), axis=1)
# y_pred = (y_pred > 0.5)
end_predict = time.time()
val_accuracy = max(history.history['val_accuracy'])
result(y_pred,y_test,"ANN",start,end_train,end_predict,val_accuracy)
pname = method +"_"+"ANN"+"_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
#plt.show()
print("ANN completed")

## **10.DNN**
multi_dnn = Sequential()
# Adding the input layer and the first hidden layer
multi_dnn.add(Dense(units = 19, kernel_initializer = 'uniform', activation = 'relu', input_dim = X_train.shape[1]))

# Adding the second hidden layer
multi_dnn.add(Dense(units = 19, kernel_initializer = 'uniform', activation = 'relu'))

# Adding the third hidden layer
multi_dnn.add(Dense(units = 19, kernel_initializer = 'uniform', activation = 'relu'))

# Adding the output layer
multi_dnn.add(Dense(units = len(np.unique(y_train)), kernel_initializer = 'uniform', activation = 'softmax'))

# Compiling the ANN | means applying SGD on the whole ANN
multi_dnn.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])

# Fitting the ANN to the Training set
start = time.time()
history = multi_dnn.fit(X_train, y_train,epochs=10, batch_size=50,verbose=2, validation_data=(X_valid,y_valid))
end_train = time.time()
y_pred = np.argmax(multi_dnn.predict(X_test), axis=1)
end_predict = time.time()

val_accuracy = max(history.history['val_accuracy'])
result(y_pred,y_test,"DNN",start,end_train,end_predict,val_accuracy)
pname = method +"_"+"DNN"+"_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
#plt.show()
print("DNN completed")


## **12.Gradient Boosting Classifier**
start = time.time()
multi_gb = GradientBoostingClassifier().fit(X_train,y_train)
end_train = time.time()
y_pred= multi_gb.predict(X_test) # These are the predictions from the test data.
end_predict = time.time()
y_val_pred = multi_gb.predict(X_valid)
val_accuracy = accuracy_score(y_valid, y_val_pred)
result(y_pred,y_test,"Gradient Boosting",start,end_train,end_predict,val_accuracy)
pname = method +"_"+"GradientBoostingClassifier"+"_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
#plt.show()
print("GradientBoostingClassifier completed")

## **13.xgboost**
#xgb_model = XGBClassifier('n_estimators': 500,'max_depth': 16)
xgb_model = XGBClassifier()
start = time.time()
xgb_model.fit(X_train, y_train)
end_train = time.time()
y_pred = xgb_model.predict(X_test)
end_predict = time.time()
y_val_pred = xgb_model.predict(X_valid)
val_accuracy = accuracy_score(y_valid, y_val_pred)
result(y_pred,y_test,"XGBoost",start,end_train,end_predict,val_accuracy)
pname = method +"_"+"XGBClassifier"+"_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
#plt.show()
print("XGBClassifier completed")

## **14.Gaussian Naive Bayes**
NB_model = GaussianNB()
start = time.time()
NB_model.fit(X_train, y_train)
end_train = time.time()
y_pred = NB_model.predict(X_test)
end_predict = time.time()
y_val_pred = NB_model.predict(X_valid)
val_accuracy = accuracy_score(y_valid, y_val_pred)
result(y_pred,y_test,"Gaussian Naive Bayes",start,end_train,end_predict,val_accuracy)
pname = method +"_"+"Gaussian_Naive_Bayes"+"_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
#plt.show()
print("Gaussian_Naive_Bayes completed")

## **15.Adaptive Gradient Boosting**
weak_learner = DecisionTreeClassifier(max_leaf_nodes=8)
n_estimators = 300

AB_model = AdaBoostClassifier(
    estimator=weak_learner,
    n_estimators=n_estimators,
    algorithm="SAMME",
    random_state=42,
)
start = time.time()
AB_model.fit(X_train, y_train)
end_train = time.time()
y_pred = AB_model.predict(X_test)
end_predict = time.time()
y_val_pred = AB_model.predict(X_valid)
val_accuracy = accuracy_score(y_valid, y_val_pred)
result(y_pred,y_test,"ADaboost",start,end_train,end_predict,val_accuracy)
pname = method +"_"+"Adaptive_Gradient_Boosting"+"_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
#plt.show()
print("Adaptive_Gradient_Boosting completed")

## **16.QuadraticDiscriminantAnalysis**
qda_multi = QuadraticDiscriminantAnalysis()
start = time.time()
qda_multi.fit(X_train,y_train)
end_train = time.time()
y_pred = qda_multi.predict(X_test)
end_predict = time.time()
y_val_pred = qda_multi.predict(X_valid)
val_accuracy = accuracy_score(y_valid, y_val_pred)
result(y_pred,y_test,"QDA",start,end_train,end_predict,val_accuracy)
pname = method +"_"+"QDA"+"_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
#plt.show()
print("QDA completed")

## **17.shallow neural network**
num_classes =  len(np.unique(y_train))
snn_multi = Sequential()
snn_multi.add(Dense(64, input_shape=(X_train.shape[1],), activation='relu'))
snn_multi.add(Dense(32, activation='relu'))
snn_multi.add(Dense(20, activation='relu'))
snn_multi.add(Dense(num_classes, activation='softmax'))


snn_multi.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

start = time.time()
history = snn_multi.fit(X_train, y_train,validation_data=(X_valid,y_valid), epochs=10, batch_size=50)
end_train = time.time()
y_pred = np.argmax(snn_multi.predict(X_test), axis=1)
end_predict = time.time()
val_accuracy = max(history.history['val_accuracy'])
result(y_pred,y_test,"SNN",start,end_train,end_predict,val_accuracy)
pname = method +"_"+"SNN"+"_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
#plt.show()
print("SNN completed")

## **18.restricted boltzmann machine**
num_classes = len(np.unique(y_train))

# Define the RBM class
class RBM(tf.keras.layers.Layer):
    def __init__(self, hidden_dim, name="rbm", **kwargs):
        super(RBM, self).__init__(name=name, **kwargs)
        self.hidden_dim = hidden_dim

    def build(self, input_shape):
        self.W = self.add_weight(shape=(input_shape[-1], self.hidden_dim), initializer='uniform', trainable=True, name='weights')
        self.h_bias = self.add_weight(shape=(self.hidden_dim,), initializer='zeros', trainable=True, name='h_bias')
        self.v_bias = self.add_weight(shape=(input_shape[-1],), initializer='zeros', trainable=True, name='v_bias')

    def call(self, inputs):
        hidden_prob = tf.nn.sigmoid(tf.matmul(inputs, self.W) + self.h_bias)
        hidden_state = self._sample_prob(hidden_prob)
        visible_prob = tf.nn.sigmoid(tf.matmul(hidden_state, tf.transpose(self.W)) + self.v_bias)
        return visible_prob, hidden_state

    def _sample_prob(self, probs):
        return tf.nn.relu(tf.sign(probs - tf.random.uniform(tf.shape(probs))))

# Assuming class 6 corresponds to index 5 (Python uses 0-based indexing)
input_data = Input(shape=(X_train.shape[1],))
rbm1_visible, rbm1_hidden = RBM(hidden_dim=128, name="rbm1")(input_data)
rbm2_visible, rbm2_hidden = RBM(hidden_dim=64, name="rbm2")(rbm1_hidden)
rbm3_visible, rbm3_hidden = RBM(hidden_dim=32, name="rbm3")(rbm2_hidden)

# Specific RBM for class 6 with more hidden units
rbm6_visible, rbm6_hidden = RBM(hidden_dim=64, name="rbm6")(rbm3_hidden)

# Classifier model
classifier_output = Dense(num_classes, activation='softmax', name='classifier')(rbm6_hidden)

# Create the combined model
model = tf.keras.Model(inputs=input_data, outputs=classifier_output)

# Compile the model
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])


# Train the stacked RBM + classifier
start = time.time()
history = model.fit(X_train, y_train, validation_data=(X_valid,y_valid),epochs=10, batch_size=50)
end_train = time.time()
y_pred = np.argmax(model.predict(X_test), axis=1)
end_predict = time.time()
val_accuracy = max(history.history['val_accuracy'])
result(y_pred,y_test,"RBM",start,end_train,end_predict,val_accuracy)
pname = method +"_"+"RBM"+"_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
#plt.show()
print("RBM completed")

## **19.LSTM**
num_classes =  len(np.unique(y_train))

# Convert DataFrame to NumPy array
X_train_array_multi = X_train.to_numpy()

# Reshape input data for LSTM
X_train_reshaped_multi = X_train_array_multi.reshape((X_train_array_multi.shape[0], X_train_array_multi.shape[1], 1))

# Define the recuurent neural network model
rnn_multi = Sequential()
rnn_multi.add(LSTM(128, input_shape=(X_train_reshaped_multi.shape[1], X_train_reshaped_multi.shape[2])))
rnn_multi.add(Dense(32, activation='relu'))
rnn_multi.add(Dense(num_classes, activation='softmax'))
rnn_multi.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

start = time.time()
history = rnn_multi.fit(X_train, y_train, epochs=10,validation_data=(X_valid,y_valid), batch_size=50)
end_train = time.time()

y_pred = np.argmax(rnn_multi.predict(X_test), axis=1)
end_predict = time.time()
val_accuracy = max(history.history['val_accuracy'])
result(y_pred,y_test,"LSTM",start,end_train,end_predict,val_accuracy)
pname = method +"_"+"LSTM"+"_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
#plt.show()
print("19 LSTM completed")

## **20.reconstruction neural networks**

num_classes = len(np.unique(y_train))

# Assuming y_train_multi is one-hot encoded
y_train_multi_onehot = tf.keras.utils.to_categorical(y_train, num_classes=num_classes)
y_test_multi_onehot = tf.keras.utils.to_categorical(y_test, num_classes=num_classes)
y_valid_multi_onehot = tf.keras.utils.to_categorical(y_valid, num_classes=num_classes)
# Define model architecture
input_dim = X_train.shape[1]
encoding_dim = 32  # Choose appropriate dimensionality
latent_dim = 2  # Dimensionality of the latent space

# Encoder
input_layer = tf.keras.layers.Input(shape=(input_dim,))
hidden = tf.keras.layers.Dense(64, activation='relu')(input_layer)
z_mean = tf.keras.layers.Dense(latent_dim)(hidden)
z_log_var = tf.keras.layers.Dense(latent_dim)(hidden)

# Reparameterization trick
def sampling(args):
    z_mean, z_log_var = args
    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0., stddev=1.)
    return z_mean + K.exp(0.5 * z_log_var) * epsilon

z = tf.keras.layers.Lambda(sampling,output_shape=(latent_dim,))([z_mean, z_log_var])

# Decoder
decoder_hidden = tf.keras.layers.Dense(64, activation='relu')(z)
output_layer = tf.keras.layers.Dense(num_classes, activation='softmax')(decoder_hidden)

# Define VAE model
vae = tf.keras.models.Model(inputs=input_layer, outputs=output_layer)

# Compile model
vae.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

start = time.time()
history = vae.fit(X_train, y_train_multi_onehot,validation_data= (X_valid,y_valid_multi_onehot), epochs=10, batch_size=50, shuffle=True)
end_train = time.time()
y_pred = np.argmax(vae.predict(X_test), axis=1)
end_predict = time.time()
val_accuracy = max(history.history['val_accuracy'])
result(y_pred,y_test,"reconstruction_NN",start,end_train,end_predict,val_accuracy)
pname = method +"_"+"reconstruction_NN"+"_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
#plt.show()
print("20 reconstruction_NN comlepeted")


X_train = train_data.drop(columns=['label'],axis=1)
X_test = test_data.drop(columns=['label'],axis=1)
y_train = train_data['label']
y_test = test_data['label']
X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=1/7, random_state=7)


## **22.DANN**

def build_dann_model(input_shape, num_classes, lambda_val=1e-3):
    input_layer = Input(shape=input_shape, name='input_layer')

    # Feature extractor
    shared_layer = Dense(128, activation='relu')(input_layer)
    shared_layer = Dropout(0.5)(shared_layer)

    # Source classifier
    source_classifier = Dense(num_classes, activation='softmax', name='source_classifier')(shared_layer)

    # Domain classifier
    domain_classifier = Dense(1, activation='sigmoid', name='domain_classifier')(shared_layer)

    # Combined model
    model = Model(inputs=input_layer, outputs=[source_classifier, domain_classifier])

    # Domain adversarial loss
    def domain_adversarial_loss(y_true, y_pred):
        return K.mean(K.binary_crossentropy(y_true, y_pred))

    # Compile model
    model.compile(optimizer=Adam(learning_rate=1e-3),
                  loss={'source_classifier': 'categorical_crossentropy', 'domain_classifier': domain_adversarial_loss},
                  loss_weights={'source_classifier': 1.0, 'domain_classifier': lambda_val},
                  metrics={'source_classifier': 'accuracy'})

    return model

# Convert class vectors to binary class matrices
num_classes = len(np.unique(y_train))
y_train_categorical = tf.keras.utils.to_categorical(y_train, num_classes)
y_test_categorical = tf.keras.utils.to_categorical(y_test, num_classes)
y_valid_categorical = tf.keras.utils.to_categorical(y_valid, num_classes)
# Build and train DANN model
input_shape = (X_train.shape[1],)
lambda_val = 1e-3  # Trade-off parameter for domain adversarial loss

dann_model = build_dann_model(input_shape, num_classes, lambda_val)
start = time.time()
dann_model.fit(X_train, {'source_classifier': y_train_categorical, 'domain_classifier': np.zeros((len(X_train), 1))},
               epochs=10, batch_size=64, validation_data=(X_valid, {'source_classifier': y_valid_categorical, 'domain_classifier': np.ones((len(X_valid), 1))}))

end_train = time.time()
predictions = dann_model.predict(X_test)
# Extracting the predicted class probabilities and domain predictions
source_classifier_predictions = predictions[0]
y_pred = np.argmax(source_classifier_predictions, axis=1)
end_predict = time.time()

valid_predictions = dann_model.predict(X_valid)
source_classifier_predictions_valid = valid_predictions[0]
y_pred_valid = np.argmax(source_classifier_predictions_valid, axis=1)

val_accuracy = accuracy_score(y_valid,y_pred_valid)
result(y_pred,y_test,"DANN",start,end_train,end_predict,val_accuracy)
pname = method +"_"+"DANN"+"_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
#plt.show()
print("22 DANN completed")

## **23.Deep brief networks (DBNs)**

# Number of classes
num_classes = len(np.unique(y_train))

# Create a pipeline with BernoulliRBM and MLPClassifier
rbm = BernoulliRBM(n_components=64, learning_rate=0.01, n_iter=20, random_state=42, verbose=True)
mlp = MLPClassifier(hidden_layer_sizes=(128,), max_iter=10, random_state=52)
start = time.time()
dbn_model = Pipeline(steps=[('rbm', rbm), ('mlp', mlp)])

# Train the classifier
dbn_model.fit(X_train, y_train)
end_train = time.time()
## Predict on the test set
y_pred = dbn_model.predict(X_test)
end_predict = time.time()
y_pred_valid = dbn_model.predict(X_valid)
val_accuracy = accuracy_score(y_valid,y_pred_valid)
result(y_pred,y_test,"DBN",start,end_train,end_predict,val_accuracy)
pname = method +"_"+"DBN"+"_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
#plt.show()
print("23 DBN completed")

## **24.Deep Boltzmann machines (DBMs)**

# Convert data to float32 to ensure consistent data types
X_train = X_train.astype(np.float32)
X_test = X_test.astype(np.float32)

X_train = X_train.values
X_test = X_test.values

# Build a simple Restricted Boltzmann Machine (RBM) using TensorFlow
class RBM(tf.Module):
    def __init__(self, visible_dim, hidden_dim, learning_rate=0.01):
        self.visible_dim = visible_dim
        self.hidden_dim = hidden_dim
        self.learning_rate = learning_rate

        # Initialize weights and biases
        self.W = tf.Variable(tf.random.normal([visible_dim, hidden_dim], stddev=0.01, dtype=tf.float32))
        self.b_visible = tf.Variable(tf.zeros([visible_dim], dtype=tf.float32))
        self.b_hidden = tf.Variable(tf.zeros([hidden_dim], dtype=tf.float32))

    def _softmax(self, x):
        exp_x = tf.exp(x)
        return exp_x / tf.reduce_sum(exp_x, axis=1, keepdims=True)

    def sample_hidden(self, visible_prob):
        hidden_prob = self._softmax(tf.matmul(visible_prob, self.W) + self.b_hidden)
        return tf.nn.relu(tf.sign(hidden_prob - tf.random.uniform(tf.shape(hidden_prob))))

    def sample_visible(self, hidden_prob):
        visible_prob = self._softmax(tf.matmul(hidden_prob, tf.transpose(self.W)) + self.b_visible)
        return tf.nn.relu(tf.sign(visible_prob - tf.random.uniform(tf.shape(visible_prob))))

    def contrastive_divergence(self, x, k=1):
        visible = x
        for _ in range(k):
            hidden = self.sample_hidden(visible)
            visible = self.sample_visible(hidden)

        positive_hidden = self._softmax(tf.matmul(x, self.W) + self.b_hidden)
        negative_hidden = self._softmax(tf.matmul(visible, self.W) + self.b_hidden)

        # Update weights and biases
        self.W.assign_add(self.learning_rate * (tf.matmul(tf.transpose(x), positive_hidden) -
                                                tf.matmul(tf.transpose(visible), negative_hidden)))
        self.b_visible.assign_add(self.learning_rate * tf.reduce_mean(x - visible, axis=0))
        self.b_hidden.assign_add(self.learning_rate * tf.reduce_mean(positive_hidden - negative_hidden, axis=0))

# Number of visible and hidden units
visible_dim = X_train.shape[1]
hidden_dim1 = 64
hidden_dim2 = 32

# Create RBMs for each layer
rbm1 = RBM(visible_dim, hidden_dim1)
rbm2 = RBM(hidden_dim1, hidden_dim2)

# Training RBMs
num_epochs = 5
batch_size = 32
start = time.time()
# Training first RBM
for epoch in range(num_epochs):
    for i in range(0, len(X_train), batch_size):
        batch_data = X_train[i:i+batch_size]
        rbm1.contrastive_divergence(batch_data)

# Getting hidden layer representation from the first RBM
hidden1_representation = tf.nn.relu(tf.sign(rbm1.sample_hidden(X_train)))

# Training second RBM using the hidden layer representation from the first RBM
for epoch in range(num_epochs):
    for i in range(0, len(hidden1_representation), batch_size):
        batch_data = hidden1_representation[i:i+batch_size]
        rbm2.contrastive_divergence(batch_data)

# Getting hidden layer representation from the second RBM
hidden2_representation = tf.nn.relu(tf.sign(rbm2.sample_hidden(hidden1_representation)))

# Fine-tuning for classification
num_classes = len(np.unique(y_train))  # Replace with the actual number of classes
dbm_model = tf.keras.Sequential([
    tf.keras.layers.Dense(hidden_dim1, activation='relu'),
    tf.keras.layers.Dense(hidden_dim2, activation='relu'),
    tf.keras.layers.Dense(num_classes, activation='softmax')
])

dbm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
start = time.time()
history = dbm_model.fit(X_train, y_train, epochs=10, batch_size=50, shuffle=True, validation_data=(X_valid,y_valid))
end_train = time.time()
## Predict on the test set
y_pred_probabilities = dbm_model.predict(X_test)
end_predict = time.time()
y_pred = np.argmax(y_pred_probabilities, axis=1)
val_accuracy = max(history.history['val_accuracy'])
result(y_pred,y_test,"DBM",start,end_train,end_predict,val_accuracy)
pname = method +"_"+"DBM"+"_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
#plt.show()
print("24 DBM completed")

## **25.DEEP AUTO ENCODERS(DA)**
num_classes = len(np.unique(y_train))

# Define the autoencoder model
autoencoder = Sequential()

# Encoder
autoencoder.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))
autoencoder.add(Dense(64, activation='relu'))
autoencoder.add(Dense(32, activation='relu'))

# Decoder
autoencoder.add(Dense(64, activation='relu'))
autoencoder.add(Dense(128, activation='relu'))
autoencoder.add(Dense(X_train.shape[1], activation='linear'))

# Compile the autoencoder
autoencoder.compile(optimizer='adam', loss='mean_squared_error')

# Train the autoencoder
autoencoder.fit(X_train, X_train, epochs=10, batch_size=50, shuffle=True, validation_data=(X_test, X_test))

# Add a classification head on top of the trained autoencoder
da_model = Sequential()
da_model.add(autoencoder.layers[0])  # Add encoder layers
da_model.add(autoencoder.layers[1])
da_model.add(autoencoder.layers[2])
da_model.add(Dense(num_classes, activation='softmax'))  # Adjust output layer for multiple classes

# Compile the classification model
da_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Convert labels to one-hot encoding
y_train_onehot = to_categorical(y_train, num_classes=num_classes)
y_test_onehot = to_categorical(y_test, num_classes=num_classes)
y_valid_onehot = to_categorical(y_valid, num_classes=num_classes)
# Train the classification model using the encoded representations
start = time.time()
history = da_model.fit(X_train, y_train_onehot, epochs=10, batch_size=32, shuffle=True, validation_data=(X_valid, y_valid_onehot))
end_train = time.time()
## Predict on the test set
y_pred_probabilities = da_model.predict(X_test)
end_predict = time.time()
val_accuracy = max(history.history['val_accuracy'])
y_pred = np.argmax(y_pred_probabilities, axis=1)
result(y_pred,y_test,"DA",start,end_train,end_predict,val_accuracy)
pname = method +"_"+"DA"+"_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
#plt.show()
print("25 DA completed")

## **26.PassiveAggressiveClassifier**
model =  PassiveAggressiveClassifier(max_iter=1000, random_state=0,tol=1e-3)
start = time.time()
model.fit(X_train, y_train)
end_train = time.time()
y_pred = model.predict(X_test)
end_predict = time.time()
y_val_pred = model.predict(X_valid)
val_accuracy = accuracy_score(y_valid, y_val_pred)
result(y_pred,y_test,"PassiveAggressiveClassifier",start,end_train,end_predict,val_accuracy)
pname = method +"_"+"PassiveAggressiveClassifier"+"_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
#plt.show()
print("26 PassiveAggressiveClassifier completed")

## **27.RidgeClassifier**
model =  RidgeClassifier()
start = time.time()
model.fit(X_train, y_train)
end_train = time.time()
y_pred = model.predict(X_test)
end_predict = time.time()
y_val_pred = model.predict(X_valid)
val_accuracy = accuracy_score(y_valid, y_val_pred)
result(y_pred,y_test,"RidgeClassifier",start,end_train,end_predict,val_accuracy)
pname = method +"_"+"RidgeClassifier"+"_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
#plt.show()
print("27 RidgeClassifier completed")

## **29.NearestCentroid**
model = NearestCentroid()
start = time.time()
model.fit(X_train, y_train)
end_train = time.time()
y_pred = model.predict(X_test)
end_predict = time.time()
y_val_pred = model.predict(X_valid)
val_accuracy = accuracy_score(y_valid, y_val_pred)
result(y_pred,y_test,"NearestCentroid",start,end_train,end_predict,val_accuracy)
pname = method +"_"+"NearestCentroid"+"_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
#plt.show()
print("NearestCentroid comlepeted")

## **29.Cost Sensitive Logistic Regression(CSLR)**
def get_sample_weight(cost_matrix, y_tru):
  y_true = np.array(y_tru)
  num_samples = len(y_true)
  sample_weights = np.zeros(num_samples)
  for i in range(num_samples):
    true_class = y_true[i]
    for j in range(len(cost_matrix)):
      if j != true_class:
        sample_weights[i] += cost_matrix[true_class, j]
  return sample_weights

cost_matrix = np.array([[0, 1, 2, 3, 4, 5, 6, 7, 8],  # Costs for misclassifying class 0
                        [1, 0, 1, 2, 3, 4, 5, 6, 7],  # Costs for misclassifying class 1
                        [2, 1, 0, 1, 2, 3, 4, 5, 6],  # Costs for misclassifying class 2
                        [3, 2, 1, 0, 1, 2, 3, 4, 5],  # Costs for misclassifying class 3
                        [4, 3, 2, 1, 0, 1, 2, 3, 4],  # Costs for misclassifying class 4
                        [5, 4, 3, 2, 1, 0, 1, 2, 3],  # Costs for misclassifying class 5
                        [6, 5, 4, 3, 2, 1, 0, 1, 2],  # Costs for misclassifying class 6
                        [7, 6, 5, 4, 3, 2, 1, 0, 1],  # Costs for misclassifying class 7
                        [8, 7, 6, 5, 4, 3, 2, 1, 0]]) # Costs for misclassifying class 8

sample_weights = get_sample_weight(cost_matrix, y_train)

clf = LogisticRegression(solver='lbfgs')
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

clf_cost_sensitive = LogisticRegression(solver='lbfgs')
start = time.time()
clf_cost_sensitive.fit(X_train, y_train, sample_weight=sample_weights)
end_train = time.time()

y_pred = clf_cost_sensitive.predict(X_test)
end_predict = time.time()
y_val_pred = clf_cost_sensitive.predict(X_valid)
val_accuracy = accuracy_score(y_valid, y_val_pred)
result(y_pred,y_test,"CSLR",start,end_train,end_predict,val_accuracy)
pname = method +"_"+"CSLR"+"_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
#plt.show()
print("CSLR comlepeted")

## **30.Cost sensitive bagging classifier(CSBC)**
class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
# Step 3: Initialize the CostSensitiveBaggingClassifier model
base_estimator = DecisionTreeClassifier(max_depth=5)
bagging_model = BaggingClassifier(base_estimator=base_estimator, n_estimators=10, random_state=42)
start = time.time()
# Step 4: Train the model on your dataset
bagging_model.fit(X_train, y_train)
end_train = time.time()
# Step 5: Evaluate the model's performance
y_pred = bagging_model.predict(X_test)
end_predict = time.time()
y_val_pred = bagging_model.predict(X_valid)
val_accuracy = accuracy_score(y_valid, y_val_pred)
result(y_pred,y_test,"CSBC",start,end_train,end_predict,val_accuracy)
pname = method +"_"+"CSBC"+"_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
#plt.show()
print("CSBC comlepeted")

## **31.lightgbm**
from lightgbm import LGBMClassifier
lgbm = LGBMClassifier()
start = time.time()
lgbm.fit(X_train, y_train)
end_train = time.time()
y_pred = lgbm.predict(X_test)
end_predict = time.time()
y_val_pred = lgbm.predict(X_valid)
val_accuracy = accuracy_score(y_valid, y_val_pred)
result(y_pred,y_test,"lightgbm",start,end_train,end_predict,val_accuracy)
pname = method +"_"+"lightgbm"+"_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
#plt.show()
print("lightgbm comlepeted")

## **32.LinearDiscriminantAnalysis(LDA)**
lda = LinearDiscriminantAnalysis(n_components=1)
start=time.time()
X_train = lda.fit_transform(X_train, y_train)
X_test = lda.transform(X_test)
classifier = RandomForestClassifier(max_depth=2, random_state=0)
classifier.fit(X_train, y_train)
end_train=time.time()
y_pred = classifier.predict(X_test)
end_predict=time.time()
y_val_pred = lgbm.predict(X_valid)
val_accuracy = accuracy_score(y_valid, y_val_pred)
result(y_pred,y_test,"LDA",start,end_train,end_predict,val_accuracy)
pname = method +"_"+"LDA"+"_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
print("LDA comlepeted")
#plt.show()

## **33.GRU**
num_classes =  len(np.unique(y_train))
# Convert DataFrame to NumPy array
X_train_array_multi = X_train
# Reshape input data for LSTM
X_train_reshaped_multi = X_train_array_multi.reshape((X_train_array_multi.shape[0], X_train_array_multi.shape[1], 1))
# Define the recuurent neural network model
rnn_multi = Sequential()
rnn_multi.add(GRU(128, input_shape=(X_train_reshaped_multi.shape[1], X_train_reshaped_multi.shape[2])))
rnn_multi.add(Dense(32, activation='relu'))
rnn_multi.add(Dense(num_classes, activation='softmax'))
rnn_multi.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

start = time.time()
rnn_multi.fit(X_train, y_train, epochs=10, batch_size=50)
end_train = time.time()
y_pred = np.argmax(rnn_multi.predict(X_test), axis=1)
end_predict = time.time()
y_pred_valid = np.argmax(rnn_multi.predict(X_valid), axis=1)
val_accuracy = accuracy_score(y_valid, y_pred_valid)
result(y_pred,y_test,"GRU",start,end_train,end_predict,val_accuracy)
pname = method +"_"+"GRU"+"_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
#plt.show()
print("GRU comlepeted")

X_train = train_data.drop(columns=['label'],axis=1)
X_test = test_data.drop(columns=['label'],axis=1)
y_train = train_data['label']
y_test = test_data['label']


X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=1/7, random_state=7)
## **34.Stochastic gradient**
sgd_multi = make_pipeline(StandardScaler(), SGDClassifier(random_state=24))
# Measure time to train
start = time.time()
sgd_multi.fit(X_train, y_train)
end_train = time.time()
# Measure time to predict
y_pred = sgd_multi.predict(X_test)
end_predict = time.time()
y_val_pred = sgd_multi.predict(X_valid)
val_accuracy = accuracy_score(y_valid, y_val_pred)
result(y_pred,y_test,"Stochastic_gradient",start,end_train,end_predict,val_accuracy)
pname = method +"_"+"Stochastic_gradient"+"_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
#plt.show()
print(" 34 Stochastic_gradient completed")



## **36.ExtraTreesClassifier**
# Train Extra Trees classifier and evaluate
start = time.time()
extra_trees_clf = ExtraTreesClassifier()
extra_trees_clf.fit(X_train, y_train)
end_train = time.time()
y_pred = extra_trees_clf.predict(X_test)
end_predict = time.time()
y_val_pred = extra_trees_clf.predict(X_valid)
val_accuracy = accuracy_score(y_valid, y_val_pred)
result(y_pred,y_test,"ExtraTreesClassifier",start,end_train,end_predict,val_accuracy)
pname = method +"_"+"ExtraTreesClassifier"+"_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
#plt.show()
print("36 ExtraTreesClassifier completed")

## **36.Feed Forward Neural Networks**
# Define the neural network architecture
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(len(np.unique(y_train)), activation='softmax')
])

# Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy']) # This accuracy is for training monitoring, not the evaluation metric

# Train the model
start = time.time()
history = model.fit(X_train, y_train, epochs=10, batch_size=50, verbose=1 ,validation_data=(X_valid,y_valid))
end_train = time.time()
y_pred_prob = model.predict(X_test)
y_pred = np.argmax(y_pred_prob, axis=1)
end_predict = time.time()
val_accuracy = max(history.history['val_accuracy'])
result(y_pred,y_test,"FFNN",start,end_train,end_predict,val_accuracy)
pname = method +"_"+"FFNN"+"_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
#plt.show()
print("37 FFNN completed")

## **38.Fuzzy**
# Train the model
start = time.time()
# Generate fuzzy c-means clusters
n_clusters = 10  # Number of classes
centers, u_train, _, _, _, _, _ = fuzz.cluster.cmeans(
    X_train.T, n_clusters, 2, error=0.005, maxiter=1000
)
end_train = time.time()
# Predict cluster membership for test data
u_test, _, _, _, _, _ = fuzz.cluster.cmeans_predict(
    X_test.T, centers, 2, error=0.005, maxiter=1000
)
# Assign class labels based on cluster membership
y_pred = np.argmax(u_test, axis=0)
end_predict = time.time()

# Predict cluster membership for test data
u_valid, _, _, _, _, _ = fuzz.cluster.cmeans_predict(
    X_valid.T, centers, 2, error=0.005, maxiter=1000
)
y_val_pred = np.argmax(u_valid,axis=0)
val_accuracy = accuracy_score(y_valid, y_val_pred)
result(y_pred,y_test,"Fuzzy",start,end_train,end_predict,val_accuracy)
pname = method +"_"+"Fuzzy"+"_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
#plt.show()
print("38 Fuzzy completed")

## **39.Ensemble_of_DL_Networks(EDLNs)**
# Define the architecture of your neural network (example architecture)
def create_model(input_shape, num_classes):
    model = keras.Sequential([
        keras.layers.Dense(64, activation='relu', input_shape=input_shape),
        keras.layers.Dense(64, activation='relu'),
        keras.layers.Dense(num_classes, activation='softmax')
    ])
    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model

# Train the model
start = time.time()
# Define hyperparameters
num_networks = 5
epochs = 10
num_classes = len(np.unique(y_train))  # Number of unique classes in the training data
# Train multiple neural networks
models = []
for i in range(num_networks):
    model = create_model(input_shape=X_train.shape[1:], num_classes=num_classes)
    model.fit(X_train, y_train, epochs=epochs, verbose=0)
    models.append(model)
end_train = time.time()
# Make predictions on test data using each model
predictions = np.array([model.predict(X_test) for model in models])
# Aggregate predictions by averaging
y_pred = np.argmax(np.mean(predictions, axis=0), axis=1)
end_predict = time.time()

predictions_valid = np.array([model.predict(X_valid) for model in models])
y_pred_valid = np.argmax(np.mean(predictions_valid, axis=0), axis=1)
val_accuracy = accuracy_score(y_valid, y_pred_valid)
result(y_pred,y_test,"EDLNs",start,end_train,end_predict,val_accuracy)

pname = method +"_"+"EDLNs"+"_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
#plt.show()
print("39 EDLNs completed")

## **40.GMM**
# Convert data to numpy arrays if needed
X_train_multi = np.array(X_train)
X_test_multi = np.array(X_test)
X_valid_multi = np.array(X_valid)
# Number of classes
n_classes = len(set(y_train))
# Dictionary to store GMMs for each class
gmm_models = {}
# Train GMMs for each class
start = time.time()
for i in range(n_classes):
    # Filter data for the current class
    X_class = X_train_multi[y_train == i]
    # Fit Gaussian Mixture Model
    gmm = GaussianMixture(n_components=2)  # You can adjust n_components as needed
    gmm.fit(X_class)
    # Store the trained GMM
    gmm_models[i] = gmm
end_train = time.time()
y_pred = []
for x in X_test_multi:
    class_likelihoods = []
    # Reshape x to have the appropriate dimensions
    x_reshaped = x.reshape(1, -1)
    # Calculate likelihood for each class
    for i in range(n_classes):
        class_likelihood = gmm_models[i].score_samples(x_reshaped)
        class_likelihoods.append(class_likelihood)
    # Assign the class with the highest likelihood
    predicted_class = max(zip(class_likelihoods, range(n_classes)))[1]
    y_pred.append(predicted_class)
end_predict = time.time()

y_valid_pred = []
for x in X_valid_multi:
    class_likelihoods = []
    # Reshape x to have the appropriate dimensions
    x_reshaped = x.reshape(1, -1)
    # Calculate likelihood for each class
    for i in range(n_classes):
        class_likelihood = gmm_models[i].score_samples(x_reshaped)
        class_likelihoods.append(class_likelihood)
    # Assign the class with the highest likelihood
    predicted_class = max(zip(class_likelihoods, range(n_classes)))[1]
    y_valid_pred.append(predicted_class)

val_accuracy = accuracy_score(y_valid, y_valid_pred)
result(y_pred,y_test,"GMM",start,end_train,end_predict,val_accuracy)
pname = method +"_"+"GMM"+"_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
#plt.show()
print("40 GMM completed")

"""##  41. **Bernoulli Naive Bayes for Multi class**"""
bnb = BernoulliNB()
# Train the classifier
start_train = time.time()
bnb.fit(X_train, y_train)
end_train = time.time()
y_pred = bnb.predict(X_test)
end_predict = time.time()
y_val_pred = bnb.predict(X_valid)
val_accuracy = accuracy_score(y_valid, y_val_pred)
result(y_pred, y_test, "Bernoulli Naive Bayes", start_train, end_train, end_predict,val_accuracy)
pname = method +"Bernoulli_Naive_Bayes_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
print("41 Bernoulli_Naive_Bayes completed")

"""## 42. Catboost"""
start_time = time.time()
catboost_model = CatBoostClassifier(random_state=42)
catboost_model.fit(X_train, y_train)
end_train = time.time()
start_predict = time.time()
y_pred = catboost_model.predict(X_test)
end_predict = time.time()
y_val_pred = catboost_model.predict(X_valid)
val_accuracy = accuracy_score(y_valid, y_val_pred)
result(y_pred, y_test, "CatBoost", start_time, end_train, end_predict,val_accuracy)
pname = method +"CatBoost_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
print("42 CatBoost completed")

"""## 43.Centralised blending"""
base_model1 = DecisionTreeClassifier(random_state=24)
base_model2 = RandomForestClassifier(random_state=24)
base_model3 = LogisticRegression(random_state=24)
start_train = time.time()
base_model1.fit(X_train, y_train)
base_model2.fit(X_train, y_train)
base_model3.fit(X_train, y_train)
end_train = time.time()
preds_val_base_model1 = base_model1.predict(X_test)
preds_val_base_model2 = base_model2.predict(X_test)
preds_val_base_model3 = base_model3.predict(X_test)
X_val_meta = np.column_stack((preds_val_base_model1, preds_val_base_model2, preds_val_base_model3))
blender = LogisticRegression(random_state=24)
blender.fit(X_val_meta, y_test)
preds_test_base_model1 = base_model1.predict(X_test)
preds_test_base_model2 = base_model2.predict(X_test)
preds_test_base_model3 = base_model3.predict(X_test)
X_test_meta = np.column_stack((preds_test_base_model1, preds_test_base_model2, preds_test_base_model3))
preds_test_meta = blender.predict(X_test_meta)
accuracy_meta = accuracy_score(y_test, preds_test_meta)
end_predict = time.time()

# Use base models to predict on validation set
preds_valid_base_model1 = base_model1.predict(X_valid)
preds_valid_base_model2 = base_model2.predict(X_valid)
preds_valid_base_model3 = base_model3.predict(X_valid)

# Stack predictions to create meta-features for validation set
X_valid_meta = np.column_stack((preds_valid_base_model1, preds_valid_base_model2, preds_valid_base_model3))

# Use blender model to predict on meta-features of validation set
preds_valid_meta = blender.predict(X_valid_meta)

# Calculate accuracy of predictions made by the blender model on validation set
accuracy_valid_meta = accuracy_score(y_valid, preds_valid_meta)
print("Validation Accuracy after Centralised Blending:", accuracy_valid_meta)

result(y_pred, y_test, "Centralised blending", start_train, end_train, end_predict,accuracy_valid_meta)

pname = method +"Centralised_blending_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
print("43 Centralised_blending completed")

"""## 44.Binary Logical Circular Neural Network (BLoCNet)"""

# Define the architecture of the BLoCNet
model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(len(np.unique(y_train)), activation='softmax')  # Multi-class classification, so softmax activation
])

# Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',  # Multi-class classification, so sparse categorical crossentropy loss
              metrics=['accuracy'])

# Train the model
start_train = time.time()
history = model.fit(X_train, y_train, epochs=10, batch_size=32,validation_data=(X_valid,y_valid))
end_train = time.time()

# Evaluate the model
start_predict = time.time()
y_pred = model.predict(X_test)
y_pred = np.argmax(y_pred, axis=1)
end_predict = time.time()

y_valid_pred = model.predict(X_valid)
y_valid_pred = np.argmax(y_valid_pred,axis=1)
# Calculate accuracy score
accuracy = metrics.accuracy_score(y_valid, y_valid_pred)
result(y_pred, y_test, "Binary Logical Circular Neural Network (BLoCNet)", start_train, end_train, end_predict,val_accuracy)

# Save the confusion matrix plot
pname = method +"BLoCNet_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
print("44 BLoCNet completed")

"""## 45.constructive_learning"""
base_model = DecisionTreeClassifier(random_state=42)
start_train = time.time()
base_model.fit(X_train, y_train)
end_train = time.time()
start_predict = time.time()
y_pred_base = base_model.predict(X_test)
end_predict = time.time()
base_accuracy = accuracy_score(y_test, y_pred_base)
best_model = base_model
best_accuracy = base_accuracy
def add_feature_and_evaluate(model, X_train, X_test, y_train, y_test, new_feature):
    # Ensure that the number of samples in the new feature matches the number of samples in X_test
    if len(new_feature) != len(X_test):
        raise ValueError("Number of samples in new feature does not match number of samples in X_test")

    # Generate a new feature with the same length as X_train and X_test
    new_feature_train = np.random.rand(len(X_train))
    new_feature_test = np.random.rand(len(X_test))
    # Add the new feature to the training set
    X_train_new = np.hstack((X_train, new_feature_train.reshape(-1, 1)))

    # Add the new feature to the testing set
    X_test_new = np.hstack((X_test, new_feature_test.reshape(-1, 1)))

    # Train the model with the new feature
    model.fit(X_train_new, y_train)

    # Evaluate the model
    y_pred = model.predict(X_test_new)
    accuracy = accuracy_score(y_test, y_pred)
    return accuracy , y_pred

new_feature  = np.random.rand(len(X_test)).reshape(len(X_test), 1)
new_accuracy,y_pred_new = add_feature_and_evaluate(base_model, X_train, X_test, y_train, y_test, new_feature)

if new_accuracy > best_accuracy:
    best_accuracy = new_accuracy
    best_model = base_model


new_feature_valid = np.random.rand(len(X_valid))
X_valid_with_new_feature = np.hstack((X_valid, new_feature_valid.reshape(-1, 1)))
preds_valid_best_model = best_model.predict(X_valid_with_new_feature)
accuracy_valid_best_model = accuracy_score(y_valid, preds_valid_best_model)
result(y_pred_new, y_test, "Constructive Learning", start_train, end_train, end_predict,accuracy_valid_best_model)
pname = method +"constructive_learning_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
print("45 constructive_learning completed")

"""## 46.Artificial Immune System (AIS)"""

class AISModel:
    def __init__(self, base_model):
        self.base_model = base_model
    def fit(self, X_train, y_train):
        self.base_model.fit(X_train, y_train)
    def predict(self, X_test):
        y_pred = self.base_model.predict(X_test)
        return y_pred
base_model = RandomForestClassifier(random_state=24)
ais_model = AISModel(base_model)
start_train = time.time()
ais_model.fit(X_train, y_train)
end_train = time.time()
y_pred = ais_model.predict(X_test)
end_predict = time.time()
y_valid_pred = ais_model.predict(X_valid)
accuracy_valid = accuracy_score(y_valid, y_valid_pred)
print("Validation Accuracy of AIS model:", accuracy_valid)
result(y_pred, y_test, "AIS", start_train, end_train, end_predict,accuracy_valid)
pname = method + "AIS_confusion_matrix.png"
cm = confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize'] = 8, 8
sns.set_style("white")
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
print("46 AIS completed")



"""## 48.GBBK Algorithm"""

X_train = train_data.drop(columns=['label'],axis=1)
X_test = test_data.drop(columns=['label'],axis=1)
y_train = train_data['label']
y_test = test_data['label']


X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=1/7, random_state=7)
start = time.time()
gbbk_model = GaussianNB()
gbbk_model.fit(X_train, y_train)
end_train = time.time()
y_pred = gbbk_model.predict(X_test)
end_predict = time.time()

start_valid_predict = time.time()
y_valid_pred = gbbk_model.predict(X_valid)
end_valid_predict = time.time()
accuracy_valid = accuracy_score(y_valid, y_valid_pred)
print("Validation Accuracy of GBBK model:", accuracy_valid)
result(y_pred, y_test, "GBBK", start, end_train, end_predict,accuracy_valid)
pname = method +"GBBK_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
print("48 GBBK completed")



"""## 51.Hidden Nave Bayes (HNB)"""
hnb_model = GaussianNB()
start_train = time.time()
hnb_model.fit(pd.DataFrame(X_train), pd.DataFrame(y_train))
end_train = time.time()
y_pred = hnb_model.predict(X_test.values)
end_predict = time.time()
y_valid_pred = hnb_model.predict(X_valid)
accuracy_valid = accuracy_score(y_valid, y_valid_pred)
print("Validation Accuracy of HNB model:", accuracy_valid)
result(y_pred, y_test, "HNB", start_train, end_train, end_predict,accuracy_valid)
pname = method +"HNB_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
print("51 HNB completed")

"""## 52.HistGradientBoostingClassifier"""
start_time = time.time()
hgb_model = HistGradientBoostingClassifier(random_state=24)
hgb_model.fit(X_train, y_train)
end_train = time.time()
y_pred = hgb_model.predict(X_test)
end_predict = time.time()
y_valid_pred = hgb_model.predict(X_valid)
accuracy_valid = accuracy_score(y_valid, y_valid_pred)
print("Validation Accuracy of HistGradientBoostingClassifier model:", accuracy_valid)
result(y_pred, y_test, "HistGradientBoostingClassifier", start_time, end_train, end_predict,accuracy_valid)
pname = method +"HistGradientBoostingClassifier_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
print("52 HistGradientBoostingClassifier completed")

"""## 53.IF (Isolation Forest)"""
start_train = time.time()
if_model = IsolationForest( n_estimators=500,
    max_samples=0.5,
    contamination=0.1,
    max_features=1.0,
    bootstrap=True,
    random_state=7,
    n_jobs=-1)
if_model.fit(X_train, y_train)
end_train = time.time()
y_pred = if_model.predict(X_test)
end_predict = time.time()
y_valid_pred = if_model.predict(X_valid)
accuracy_valid = accuracy_score(y_valid, y_valid_pred)
print("Validation Accuracy of IF (Isolation Forest) model:", accuracy_valid)
result(y_pred, y_test, "IF (Isolation Forest)", start_train, end_train, end_predict,accuracy_valid)
pname = method +"IF_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
print("53 IF completed")


"""## 55.Independent Component Analysis (ICA)"""
ica = FastICA(n_components=10, random_state=42)
X_train_ica = ica.fit_transform(X_train)
X_test_ica = ica.transform(X_test)
X_valid_ica = ica.transform(X_valid)
scaler = StandardScaler()
X_train_ica = scaler.fit_transform(X_train_ica)
X_test_ica = scaler.transform(X_test_ica)
X_valid_ica = scaler.transform(X_valid_ica)
rf_model = RandomForestClassifier(random_state=24)
start_train = time.time()
rf_model.fit(X_train_ica, y_train)
end_train = time.time()
y_pred = rf_model.predict(X_test_ica)
end_predict = time.time()
y_valid_pred = rf_model.predict(X_valid_ica)
accuracy_valid = accuracy_score(y_valid, y_valid_pred)
print("Validation Accuracy of ICA model:", accuracy_valid)
result(y_pred, y_test, "ICA", start_train, end_train, end_predict,accuracy_valid)
pname = method +"ICA_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
print("55 ICA completed")

"""## 56.Lasso Regression"""
lasso_model = LogisticRegression(penalty='l1', solver='saga', random_state=24, max_iter=1000)  # L1 penalty for Lasso
start_train = time.time()
lasso_model.fit(X_train, y_train)
end_train = time.time()
y_pred = lasso_model.predict(X_test)
end_predict = time.time()
y_valid_pred = lasso_model.predict(X_valid)
accuracy_valid = accuracy_score(y_valid, y_valid_pred)
print("Validation Accuracy of Lasso Regression model:", accuracy_valid)
result(y_pred, y_test, "Lasso Regression", start_train, end_train, end_predict,accuracy_valid)
pname = method +"Lasso_Regression_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
print("56 Lasso_Regression completed")

"""## 57.Meta (KNN) with Neighbors (K)"""
k = 5
base_classifier = KNeighborsClassifier(n_neighbors=k)
meta_knn_model = BaggingClassifier(base_classifier, n_estimators=10, random_state=42)
start_train = time.time()
meta_knn_model.fit(X_train, y_train)
end_train = time.time()
y_pred = meta_knn_model.predict(X_test)
end_predict = time.time()
y_valid_pred = meta_knn_model.predict(X_valid)
accuracy_valid = accuracy_score(y_valid, y_valid_pred)
print("Validation Accuracy of Meta_KNN Regression model:", accuracy_valid)
result(y_pred, y_test, "Meta_KNN", start_train, end_train, end_predict,accuracy_valid)
pname = method + "Meta_KNN_Confusion_Matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
print("57 Meta_KNN completed")




"""## 60.Projected Gradient Descent (PGD)"""
# Define attack categories
attack_categories = {
    0: 'Normal',
    1: 'Generic',
    2: 'Exploits',
    3: 'Fuzzers',
    4: 'DoS',
    5: 'Reconnaissance',
    6: 'Analysis',
    7: 'Backdoor',
    8: 'Shellcode',
    9: 'Worms'
}

# Convert labels to numeric representation
y_train_encoded = pd.factorize(y_train)[0]
y_test_encoded = pd.factorize(y_test)[0]
y_valid_encoded = pd.factorize(y_valid)[0]

# Convert data to PyTorch tensors
X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train_encoded, dtype=torch.long)
X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)
X_valid_tensor = torch.tensor(X_valid.values, dtype=torch.float32)

# Create DataLoader for training
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

# Define the neural network model
class SimpleNN(nn.Module):
    def __init__(self, input_size, output_size):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(input_size, 128)
        self.fc2 = nn.Linear(128, output_size)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Instantiate the model
input_size = X_train.shape[1]
output_size = len(np.unique(y_train))
model = SimpleNN(input_size, output_size)

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training the model
start_train = time.time()
for epoch in range(10):
    for inputs, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
end_train = time.time()

# Evaluation on test data
model.eval()
start_predict = time.time()
with torch.no_grad():
    y_pred = model(X_test_tensor).argmax(dim=1).detach().numpy()
end_predict = time.time()

# Convert predicted labels back to original attack categories
y_pred_labels = pd.Series(y_pred).map(attack_categories)
y_pred_encoded = pd.factorize(y_pred_labels)[0]

# Calculate accuracy on test data
accuracy_test = accuracy_score(y_test_encoded, y_pred_encoded)
print("Test Accuracy of Projected Gradient Descent (PGD) model:", accuracy_test)

# Confusion matrix for test data
cm = confusion_matrix(y_test_encoded, y_pred_encoded)
plt.figure(figsize=(8, 8))
sns.set_style("white")
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix - Test Data")
plt.savefig("PGD_confusion_matrix_test.png")

# Evaluation on validation data
with torch.no_grad():
    y_valid_pred = model(X_valid_tensor).argmax(dim=1).detach().numpy()

# Convert predicted labels back to original attack categories
y_valid_pred_labels = pd.Series(y_valid_pred).map(attack_categories)
y_valid_pred_encoded = pd.factorize(y_valid_pred_labels)[0]

# Calculate accuracy on validation data
accuracy_valid = accuracy_score(y_valid_encoded, y_valid_pred_encoded)
print("Validation Accuracy of Projected Gradient Descent (PGD) model:", accuracy_valid)

# Confusion matrix for validation data
cm_valid = confusion_matrix(y_valid_encoded, y_valid_pred_encoded)
plt.figure(figsize=(8, 8))
sns.set_style("white")
disp_valid = ConfusionMatrixDisplay(confusion_matrix=cm_valid)
disp_valid.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix - Validation Data")
plt.savefig("PGD_confusion_matrix_validation.png")

# Call the result function
result(y_pred_encoded, y_test_encoded, "Projected Gradient Descent (PGD)", start_train, end_train, end_predict, accuracy_valid)


print("60 Projected Gradient Descent (PGD) completed")

"""## 61.Principal Component Analysis (PCA)"""
label_encoder = LabelEncoder()
y_test_encoded = label_encoder.fit_transform(y_test)
y_valid_encoded = label_encoder.fit_transform(y_valid)
pca = PCA(n_components=0.95, random_state=42)  # Retain 95% of variance
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)
dt_multi = DecisionTreeClassifier(random_state=24)
start_train = time.time()
dt_multi.fit(X_train_pca, y_train)
end_train = time.time()
y_pred = dt_multi.predict(X_test_pca)
end_predict = time.time()
X_valid_pca = pca.transform(X_valid)
y_valid_pred = dt_multi.predict(X_valid_pca)
accuracy_valid = accuracy_score(y_valid_encoded, y_valid_pred)
print("Validation Accuracy of PCA and Decision Tree model:", accuracy_valid)
result(y_pred, y_test_encoded, "PCA", start_train, end_train, end_predict,accuracy_valid)
pname = method +"PCA_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test_encoded, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test_encoded, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
print("61 PCA completed")

"""## 62.Radial Basis Function (RBF)"""
svc_model = SVC(kernel='rbf', random_state=24)
start_train = time.time()
svc_model.fit(X_train, y_train)
end_train = time.time()
y_pred = svc_model.predict(X_test)
end_predict = time.time()

y_valid_pred = svc_model.predict(X_valid)
accuracy_valid = accuracy_score(y_valid, y_valid_pred)
print("Validation Accuracy of Radial Basis Function (RBF) SVC model:", accuracy_valid)

result(y_pred, y_test, "Radial Basis Function (RBF)", start_train, end_train, end_predict,accuracy_valid)
pname = method +"RBF_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
print("62 RBF completed")


"""## 64.Simulated Annealing (SA)"""
X_train = np.array(X_train)
y_train = np.array(y_train)
X_test = np.array(X_test)
y_test = np.array(y_test)
X_valid = np.array(X_valid)
y_valid = np.array(y_valid)
if X_train.ndim > 2:
    X_train = X_train.reshape(X_train.shape[0], -1)
if X_test.ndim > 2:
    X_test = X_test.reshape(X_test.shape[0], -1)
def objective_function(params):
    max_depth, min_samples_split, min_samples_leaf = params
    dt_model = DecisionTreeClassifier(max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, random_state=24)
    dt_model.fit(X_train, y_train)
    y_pred = dt_model.predict(X_test)
    accuracy = np.mean(y_pred == y_test)
    return -accuracy
space = [(1, 20),  # max_depth
         (2, 20),  # min_samples_split
         (1, 20)]  # min_samples_leaf
def simulated_annealing(objective_function, space, n_calls=50, initial_temperature=100.0, cooling_rate=0.95):
    best_params = None
    best_score = float('-inf')
    temperature = initial_temperature

    current_params = [np.random.randint(low, high + 1) for low, high in space]

    for _ in range(n_calls):
        next_params = [np.random.randint(low, high + 1) for low, high in space]

        current_score = objective_function(current_params)
        next_score = objective_function(next_params)

        if next_score > current_score or np.random.rand() < np.exp((next_score - current_score) / temperature):
            current_params = next_params
            current_score = next_score

        if current_score > best_score:
            best_params = current_params
            best_score = current_score

        temperature *= cooling_rate

    return best_params

start_train = time.time()
best_params = simulated_annealing(objective_function, space)
end_train = time.time()
dt_model_final = DecisionTreeClassifier(max_depth=best_params[0], min_samples_split=best_params[1], min_samples_leaf=best_params[2], random_state=24)
dt_model_final.fit(X_train, y_train)
y_pred_final = dt_model_final.predict(X_test)
end_predict = time.time()
y_test = y_test.ravel()
y_pred_final = y_pred_final.ravel()
y_pred_valid = dt_model_final.predict(X_valid)
validation_accuracy = np.mean(y_pred_valid == y_valid)
print(f"Validation Accuracy: {validation_accuracy}")
result(y_pred_final, y_test, "Simulated_Annealing", start_train, end_train, end_predict, validation_accuracy)
pname = method + "Simulated_Annealing_confusion_matrix.png"
cm = confusion_matrix(y_test, y_pred_final)
plt.rcParams['figure.figsize'] = 8, 8
sns.set_style("white")
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
print("64 Simulated_Annealing completed")


"""## 66.Stacking Classifier"""
base_classifiers = [
    ('dt', DecisionTreeClassifier(random_state=24)),
    ('rf', RandomForestClassifier(random_state=24)),
    ('knn', KNeighborsClassifier())
]
stacking_classifier = StackingClassifier(estimators=base_classifiers, final_estimator=DecisionTreeClassifier())

X_train_flattened = X_train.reshape(X_train.shape[0], -1)
X_test_flattened = X_test.reshape(X_test.shape[0], -1)
X_valid_flattened = X_valid.reshape(X_valid.shape[0],-1)

start_train = time.time()
stacking_classifier.fit(X_train, y_train)
end_train = time.time()

y_pred = stacking_classifier.predict(X_test_flattened)
end_predict = time.time()

y_valid_pred = stacking_classifier.predict(X_valid_flattened)
val_accuracy = accuracy_score(y_valid,y_valid_pred)
result(y_pred, y_test, "Stacking_Classifier", start_train, end_train, end_predict,val_accuracy)
pname = method +"Stacking_Classifier_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
print("66 Stacking_Classifier completed")

"""## 67.Stacking Dilated Convolutional Autoencoders"""
base_model = make_pipeline(StandardScaler(), DecisionTreeClassifier(random_state=24))

# Define the stacking classifier
stacking_model = StackingClassifier(estimators=[('dt', base_model)], final_estimator=DecisionTreeClassifier())

# Reshape input data to have two dimensions
X_train_flattened = X_train.reshape(X_train.shape[0], -1)
X_test_flattened = X_test.reshape(X_test.shape[0], -1)
X_valid_flattened  = X_valid.reshape(X_valid.shape[0],-1)
# Training the stacking model
start_train = time.time()
stacking_model.fit(X_train, y_train)
end_train = time.time()
y_pred = stacking_model.predict(X_test_flattened)
end_predict = time.time()
y_valid_pred = stacking_model.predict(X_valid_flattened)
val_accuracy = accuracy_score(y_valid,y_valid_pred)
result(y_pred, y_test, "Stacking_Dilated_Convolutional_Autoencoders", start_train, end_train, end_predict,val_accuracy)
pname = method +"Stacking_Dilated_Convolutional_Autoencoders_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
print("67 Stacking_Dilated_Convolutional_Autoencoders completed")

"""## 68.Temporal Deep Feedforward Neural Network"""

# Define the structure of the Temporal DNN model
y_train = pd.Series(y_train)
num_classes = y_train.nunique()
model = Sequential([
    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    Dropout(0.5),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(32, activation='relu'),
    Dropout(0.5),
    Dense(num_classes, activation='softmax')  # Assuming len(y_train.unique()) is the number of classes
])

# Compile the model
model.compile(optimizer=Adam(),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Train the model
start_train = time.time()
history = model.fit(X_train, y_train, epochs=10, batch_size=32)
end_train = time.time()

# Predict using the trained model
start_predict = time.time()
y_pred_probs = model.predict(X_test)
y_pred = np.argmax(y_pred_probs, axis=1)
end_predict = time.time()

y_pred_valid = model.predict(X_valid)
y_pred_valid = np.argmax(y_pred_valid,axis=1)
val_accuracy = accuracy_score(y_valid,y_pred_valid)
result(y_pred, y_test, "Temporal_Deep_Feedforward_Neural_Network", start_train, end_train, end_predict,val_accuracy)
pname = method +"Temporal_Deep_Feedforward_Neural_Network_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
print("68 Temporal_Deep_Feedforward_Neural_Network completed")

"""## 69. Voting Classifier"""
X_train = np.array(X_train)
y_train = np.array(y_train)
X_test = np.array(X_test)

dt1 = DecisionTreeClassifier(random_state=24)
dt2 = DecisionTreeClassifier(random_state=42)

voting_clf = VotingClassifier(estimators=[
    ('dt1', dt1),
    ('dt2', dt2),
    # Add more classifiers here if needed
], voting='hard')  # You can use 'soft' voting if your classifiers support probability prediction

start_train = time.time()
voting_clf.fit(X_train, y_train)
end_train = time.time()
start_predict = time.time()
y_pred = voting_clf.predict(X_test)
end_predict = time.time()
y_test = y_test[:len(X_test)].reshape(-1)
y_pred = np.ravel(y_pred)


y_pred_valid = voting_clf.predict(X_valid)
validation_accuracy = accuracy_score(y_valid, y_pred_valid)

result(y_pred, y_test, "Voting Classifier", start_train, end_train, end_predict,validation_accuracy)

pname = method + "Voting_Classifier_confusion_matrix.png"
cm = confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize'] = 8, 8
sns.set_style("white")
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
print("69 Voting_Classifier completed")

"""## 70.GBT"""
start_train = time.time()
gbt_model = GradientBoostingClassifier(random_state=24)
gbt_model.fit(X_train, y_train)
end_train = time.time()
y_pred = gbt_model.predict(X_test)
end_predict = time.time()
y_pred_valid = gbt_model.predict(X_valid)
val_accuracy = accuracy_score(y_valid,y_pred_valid)
result(y_pred, y_test, "GBT", start_train, end_train, end_predict,val_accuracy)
pname = method +"GBT_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
print("70 GBT completed")

"""## 71.Classification_and_Regression_Trees_CART"""

class Node:
    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):
        self.feature_index = feature_index  # Index of feature to split on
        self.threshold = threshold  # Threshold value to split on
        self.left = left  # Left subtree
        self.right = right  # Right subtree
        self.value = value  # Class label (for leaf nodes)

class CART:
    def __init__(self, max_depth=None):
        self.max_depth = max_depth

    def fit(self, X, y):
        self.n_classes = len(set(y))
        self.n_features = X.shape[1]
        self.tree = self._build_tree(X, y)

    def _build_tree(self, X, y, depth=0):
        n_samples, n_features = X.shape
        n_labels = len(set(y))

        # Stop conditions
        if depth == self.max_depth or n_labels == 1:
            value = max(set(y), key=list(y).count)
            return Node(value=value)

        # Find best split
        best_gini = float('inf')
        best_feature_index = None
        best_threshold = None

        for feature_index in range(n_features):
            thresholds = sorted(set(X[:, feature_index]))
            for threshold in thresholds:
                left_indices = (X[:, feature_index] <= threshold)
                right_indices = (X[:, feature_index] > threshold)

                left_gini = self._gini(y[left_indices])
                right_gini = self._gini(y[right_indices])

                gini = (len(left_indices) * left_gini + len(right_indices) * right_gini) / n_samples

                if gini < best_gini:
                    best_gini = gini
                    best_feature_index = feature_index
                    best_threshold = threshold

        # Split data
        left_indices = (X[:, best_feature_index] <= best_threshold)
        right_indices = (X[:, best_feature_index] > best_threshold)

        left_subtree = self._build_tree(X[left_indices], y[left_indices], depth + 1)
        right_subtree = self._build_tree(X[right_indices], y[right_indices], depth + 1)

        return Node(best_feature_index, best_threshold, left_subtree, right_subtree)

    def _gini(self, y):
        n_samples = len(y)
        gini = 1.0
        for label in set(y):
            proportion = (y == label).sum() / n_samples
            gini -= proportion ** 2
        return gini

    def predict(self, X):
        return [self._predict_one(sample, self.tree) for sample in X]

    def _predict_one(self, sample, node):
        if node.value is not None:
            return node.value

        if sample[node.feature_index] <= node.threshold:
            return self._predict_one(sample, node.left)
        else:
            return self._predict_one(sample, node.right)

# Convert X_train and y_train to NumPy arrays
X_train = np.array(X_train)
y_train = np.array(y_train)
X_test = np.array(X_test)
X_valid = np.array(X_valid)
y_valid = np.array(y_valid)
start_train = time.time()
cart_model = CART(max_depth=1)
cart_model.fit(X_train, y_train)
end_train = time.time()
start_predict = time.time()
y_pred = cart_model.predict(X_test)
end_predict = time.time()

y_pred_valid = cart_model.predict(X_valid)
val_accuracy = accuracy_score(y_valid,y_pred_valid)
result(y_pred, y_test, "CART", start_train, end_train, end_predict,val_accuracy)
pname = method +"CART_confusion_matrix.png"
cm = multilabel_confusion_matrix(y_test, y_pred)
plt.rcParams['figure.figsize']=8,8
sns.set_style("white")
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.savefig(pname)
print("71 CART completed")

## **Closing the file**
file.close()
